<?xml version="1.0"?>
<!--
  Licensed to the Apache Software Foundation (ASF) under one or more
  contributor license agreements.  See the NOTICE file distributed with
  this work for additional information regarding copyright ownership.
  The ASF licenses this file to You under the Apache License, Version 2.0
  (the "License"); you may not use this file except in compliance with
  the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>

<property>
  <name>dfs.permissions.superusergroup</name>
  <value>hadoop</value>
  <!-- 配置HDFS的超级用户组-->
</property>

<property>
  <name>dfs.namenode.name.dir</name>
  <value>file:///data/dfs/nn,file:///data01/dfs/nn,file:///data02/dfs/nn,file:///data03/dfs/nn</value>
  <!--配置NameNode所使用的本地目录-->
</property>

<property>
  <name>dfs.datanode.data.dir</name>
  <value>file:///data/dfs/dn,file:///data01/dfs/dn,file:///data02/dfs/dn,file:///data03/dfs/dn,file:///data04/dfs/dn,file:///data05/dfs/dn,file:///data06/dfs/dn,file:///data07/dfs/dn,file:///data08/dfs/dn,file:///data09/dfs/dn,file:///data10/dfs/dn,file:///data11/dfs/dn</value>
  <!--配置DataNode所使用的本地目录-->
</property>

<property>
  <name>dfs.datanode.failed.volumes.tolerated</name>
  <value>6</value>
  <!--DataNode能够容忍的出错的本地目录的数量，超出该值，datanode将shutdown-->
</property>

<property>
  <name>dfs.datanode.fsdataset.volume.choosing.policy</name>
  <value>org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy</value>
  <!--开启DataNode的负载均衡-->
</property>

<property>
  <name>dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold</name>
  <value>10737418240</value>
  <!--默认值为10737418240 (10 GB).即不同磁盘之间可用的空闲空间的大小相差10G时，认为两个磁盘数据不均衡了-->
</property>

<property>
  <name>dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction</name>
  <value>0.75</value>
  <!--默认值为0.75即写入到具有更多空闲空间的磁盘的block的数量占要写入的总的block数量的百分比为75%-->
</property>

<property>
  <name>dfs.client.read.shortcircuit</name>
  <value>true</value>
</property>
<property>
  <name>dfs.domain.socket.path</name>
  <value>/var/run/hadoop-hdfs/dn._PORT</value>
</property>

<property>
  <name>dfs.webhdfs.enabled</name>
  <value>true</value>
  <!--由于HUE会用到WebHDFS，所以需要开启WebHDFS-->
</property>

<property>
  <name>dfs.nameservices</name>
  <value>ptmind-cluster</value>
</property>

<property>
  <name>dfs.ha.namenodes.ptmind-cluster</name>
  <value>nn1,nn2</value>
</property>

<property>
  <name>dfs.namenode.rpc-address.ptmind-cluster.nn1</name>
  <value>mn-5-19.ptfuture.com:8020</value>
</property>

<property>
  <name>dfs.namenode.http-address.ptmind-cluster.nn1</name>
  <value>mn-5-19.ptfuture.com:50070</value>
</property>

<property>
  <name>dfs.namenode.rpc-address.ptmind-cluster.nn2</name>
  <value>sn-5-20.ptfuture.com:8020</value>
</property>

<property>
  <name>dfs.namenode.http-address.ptmind-cluster.nn2</name>
  <value>sn-5-20.ptfuture.com:50070</value>
</property>

<property>
  <name>dfs.namenode.shared.edits.dir</name>
<!--  <value>qjournal://dn-5-21.ptfuture.com:8485;dn-5-22.ptfuture.com:8485;dn-5-23.ptfuture.com:8485,dn-5-24.ptfuture.com:8485,dn-5-25.ptfuture.com:8485/ptmind-cluster</value>-->
  <value>qjournal://dn-5-21.ptfuture.com:8485;dn-5-22.ptfuture.com:8485;dn-5-23.ptfuture.com:8485/ptmind-cluster</value>
</property>

<property>
  <name>dfs.journalnode.edits.dir</name>
  <value>/data/dfs/jn</value>
</property>

<property>
  <name>dfs.client.failover.proxy.provider.ptmind-cluster</name>
  <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  <!-- dfs.ha.fencing.methods 指定当发生Faioverl时，对Active的NameNode进行隔离的策略，以防止数据的不一致-->
</property>

<property>
  <name>dfs.ha.fencing.methods</name>
  <value>sshfence(hdfs)</value>
</property>

<!--
<property>
  <name>dfs.ha.fencing.methods</name>
  <value>shell(/bin/true)</value>
</property>
-->
<property>
  <name>dfs.ha.fencing.ssh.connect-timeout</name>
  <value>30000</value>
</property>

<property>
  <name>dfs.ha.fencing.ssh.private-key-files</name>
  <value>/var/lib/hadoop-hdfs/.ssh/id_rsa</value>
</property>

<property>
  <name>dfs.ha.automatic-failover.enabled</name>
  <value>true</value>
  <!--开启auto failover切换-->
</property>

<property>
  <name>dfs.client.read.shortcircuit</name>
  <value>true</value>
  <!--使得impala能够直接查询本地数据-->
</property>

<property>
  <name>dfs.client.file-block-storage-locations.timeout</name>
  <value>10000</value>
</property>

<property>
  <name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
  <value>true</value>
  <!--块位置跟踪-->
</property>

<property>
 <name>dfs.replication</name>
 <value>3</value>
</property>

<property>
<name>dfs.blocksize</name>
<value>134217728</value>
</property>

<property>
<name>dfs.balance.bandwidthPerSec</name>
<value>52428800</value>
</property>

<property>
<name>fs.permissions.umask-mode</name>
<value>002</value>
</property>

<!-- dfs.datanode.max.xcievers has Deprecated;should use the new dfs.datanode.max.transfer.threads
 <property>
   <name>dfs.datanode.max.xcievers</name>
   <value>5000</value>
 </property>
-->

 <property>
    <name>dfs.datanode.max.transfer.threads</name>
    <value>20000</value>
 </property>



<property>
<name>dfs.qjournal.start-segment.timeout.ms</name>
<value>20000000</value>
</property>

<property>
<name>dfs.qjournal.prepare-recovery.timeout.ms</name>
<value>20000000</value>
</property>

<property>
<name>dfs.qjournal.accept-recovery.timeout.ms</name>
<value>20000000</value>
</property>

<property>
<name>dfs.qjournal.prepare-recovery.timeout.ms</name>
<value>20000000</value>
</property>

<property>
<name>dfs.qjournal.accept-recovery.timeout.ms</name>
<value>20000000</value>
</property>

<property>
<name>dfs.qjournal.finalize-segment.timeout.ms</name>
<value>20000000</value>
</property>

<property>
<name>dfs.qjournal.select-input-streams.timeout.ms</name>
<value>20000000</value>
</property>

<property>
<name>dfs.qjournal.get-journal-state.timeout.ms</name>
<value>20000000</value>
</property>

<property>
<name>dfs.qjournal.new-epoch.timeout.ms</name>
<value>20000000</value>
</property>

<property>
<name>dfs.qjournal.write-txns.timeout.ms</name>
<value>20000000</value>
</property>

</configuration>
